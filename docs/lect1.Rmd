---
title: 'Эконометрика #1'
author: "Mikhail Plekhanov"
date: "13.04.2015"
output: html_document
---

## Эконометрика на одном слайде

Вопросы:

- Как устроен мир? Как переменная $x$ влияет на переменную $y$?
- Что будет завтра? Как спрогнозировать переменную $y$?

Ответы на эти вопросы мы получаем с помощью моделей.

**Модель** - формула для объясняемой переменной.

Например:
$$
y_{i} = \beta_{1} +  \beta_{2}x_{i} + \epsilon_{i}
$$

## Основные типы данных

- временные ряды
- перекрестные данные
- панельные данные

Временные ряды:

```{r kable, echo=FALSE}
library(knitr)
year <- c(2010, 2011, 2012, 2013)
popul <- c(142962, 142914, 143103, 143395)
unempl <- c(7.4, 6.5, 5.5, 5.5)
df = data.frame(year, popul, unempl)
colnames(df) <- c('Год', 'Население (тыс. чел.)', 'Безработица (%)')
kable(df)
```

Перекрестная выборка:

<!-- TODO/FIXME Таблица с олимпиадой -->

Панельные данные -  сочетание первых двух.

## Данные - обозначения

- одна зависимая (объясняемая) переменная: $y$
- несколько регрессоров (объясняющих переменных): $x$, $z$...
- по каждой переменной $n$ наблюдение: $y_1$, $y_2$, ... , $y_n$

## Данные - пример

Исторические данные 1920-x годов

```{r, echo=FALSE}
head(cars)
```

## Всегда изображайте данные!

Никакой эконометрический анализ не заменит простого графического анализа. Вы можете выявить простым графическим анализом то, что выявить вслепую без графиков эконометрически очень сложно.

```{r, echo=FALSE}
plot(cars$speed, cars$dist, main='Данные по машинах 1920-х годов', ylab = 'Длина тормозного пути (м)', xlab='Скорость машины (км/ч')
grid()
```

Видим, чем больше скорость, тем больше длина тормозного пути.

## Модель

Пример: $y_{i} = \beta_{1} + \beta_{2}x_{i}+\epsilon_{i}$

- наблюдаемы переменные: $y$, $x$
- неизвестные параметры: $\beta_1$, $\beta_2$
- случайная составляющая, ошибка: $\epsilon$

План действий:

- построить адекватную модель
- получить оценки неизвестных параметров: $\hat{\beta_{1}}$, $\hat{\beta_{2}}$
- прогнозировать, заменив неизвестные параметры на оценки: $\hat{y_{i}} = \hat{\beta_{1}} + \hat{\beta_{2}}$

## Метод наименьших квадратов

Это способ получить оценки неизвестных параметров модели, исходя из реальных данных. Если мы получили какие-то оценки $\hat{\beta_{1}}$, $\hat{\beta_{2}}$, то естественно у нас возникает такое понятие как *ошибка прогноза*:

$$\hat{\epsilon_{i}} = y_{i} - \hat{y_{i}}$$

$\hat{\epsilon_{i}}$ - это разница между фактическим наблюдением $y_{i}$ и прогнозом $\hat{y_{i}}$.

И, естесственно, возникает суммарная ошибка прогноза. Чтобы ошибки не компенсировали друг друга, возведем их в квадрат и посчитаем *сумму квадратов ошибок прогноза*:

$$Q(\hat{\beta_{1}}, \hat{\beta_{2}}) = \sum\limits_{i=1}^n \hat{\epsilon_{i}}^2 = \sum\limits_{i=1}^n (y_{i} - \hat{y_{i}})^2$$

Суть МНК: возьмите в качестве оценок такие $\hat{\beta_{1}}$, $\hat{\beta_{2}}$, при которых сумма квадратов ошибок прогноза $Q$ минимальна.


#Примеры
## Пример 1. Регрессия на константу.

```{r}
# вес, кг
y <- c(60, 70, 80)
# рост, см
x <- c(170, 170, 181)
```

Оценим 2 модели:

M1: $y_{i} = \beta + \epsilon_{i}$

M2: $y_{i} = \beta_{1} + \beta_{2}x_{i}+\epsilon_{i}$

С помощью метода наименьших квадратов нам надо получить $\hat{\beta}$, $\hat{\beta_{1}}$ и $\hat{\beta_{2}}$. Первая модель предполагает, что вес не зависит от роста. Вторая модель, предполагает, что рост зависит от веса линейно.

МНК - минимизировать величину RSS:

$$\min\sum\limits_{i=1}^n \hat{\epsilon_{i}}^2 = \min\sum\limits_{i=1}^n (y_{i} - \hat{y_{i}})^2$$

**M1**: $\hat{y_{i}} = \hat{\epsilon}$, следовательно 

$$\sum\limits_{i=1}^3 (y_{i} - \hat{y_{i}})^2 = \sum\limits_{i=1}^3 (y_{i} - \hat{\beta})^2 = Q(\hat{\beta}) = \sum\limits_{i=1}^3 (y_{i}^2 - 2y_{i}\hat{\beta} + \hat{\beta}^2) = 
\sum\limits_{i=1}^3 y_{i}^2 - \sum\limits_{i=1}^3 2y_{i}\hat{\beta} + \sum\limits_{i=1}^3\hat{\beta}^2$$

$$\sum\limits_{i=1}^3\hat{\beta}^2 = 3\hat{\beta}^2$$ (В общем случае $n\hat{\beta}^2)$

$$\sum\limits_{i=1}^n 2y_{i}\hat{\beta} = 2\hat{\beta}\sum\limits_{i=1}^n y_{i}$$

$$Q(\hat{\beta}) = \sum\limits_{i=1}^n y_{i}^2 - 2\hat{\beta}\sum\limits_{i=1}^n y_{i} + n\hat{\beta}^2$$

$$Q'(\hat{\beta}) = - 2\sum\limits_{i=1}^n y_{i} + 2n\hat{\beta}$$

$$\hat{\beta} = \sum\limits_{i-1}^n \frac{y_{i}}{n} = \bar{y}$$

Таким образом: $$\hat{\beta}_{МНК} = \frac{60+70+80}{3} = 70$$

## Пример 2. Парная регрессия.

Заметим, что $$\frac{\sum\limits_{i=1}^n x_{i}}{n} = \bar{x}$$
Отсюда следует, что $$\sum\limits_{i=1}^n x_{i} = n\bar{x} = \sum\limits_{i=1}^n \bar{x}$$
и, $$\sum\limits_{i=1}^n (x_{i} - \bar{x}) = 0$$

**M2**: $y_{i} = \hat{\beta_{1}} + \hat{\beta_{2}}x_{i}$

$$RSS = \sum\limits_{i=1}^n (y_{i} - \hat{y_{i}})^2 = \sum\limits_{i=1}^n (y_{i} - \hat{\beta_{1}} - \hat{\beta_{2}}x_{i})^2 = Q(\hat{\beta_{1}}, \hat{\beta_{2}})$$

<!-- TODO/IMPL дифференцирование Q -->

Получаем:
$$\hat{\beta_{1}} = \bar{y} - \hat{\beta_{2}}\bar{x}$$
$$\hat{\beta_{2}} = \frac{\sum\limits_{i=1}^n (y_{i} - \bar{y})(x_{i}-\bar{x})}{\sum\limits_{i=1}^n (x_{i}-\bar{x})^2}$$

```{r}
beta2 = sum((y - mean(y))*(x - mean(x)))/sum((x-mean(x))^2)
beta1 = mean(y) - beta2*mean(x)
```

Получили, $\hat{\beta_{1}} = `r beta1`$ и $\hat{\beta_{2}} = `r beta2`$